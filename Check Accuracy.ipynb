{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321e3d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\POS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1327018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "hanifnoerr = \"hanifnoerr/Fine-tuned-Indonesian-Sentiment-Classifier\"\n",
    "hanifnoerr_model = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=hanifnoerr,\n",
    "    tokenizer=hanifnoerr,\n",
    "    truncation=True,\n",
    "     max_length=512\n",
    ")\n",
    "\n",
    "crypter70 = \"crypter70/IndoBERT-Sentiment-Analysis\"\n",
    "crypter70_model = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=crypter70,\n",
    "    tokenizer=crypter70,\n",
    "    truncation=True,\n",
    "     max_length=512\n",
    ")\n",
    "\n",
    "w11wo = \"w11wo/indonesian-roberta-base-sentiment-classifier\"\n",
    "w11wo_model = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=w11wo,\n",
    "    tokenizer=w11wo,\n",
    "    truncation=True,\n",
    "     max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad476a7",
   "metadata": {},
   "source": [
    "HANIFNOERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "329c16ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\POS\\AppData\\Local\\Temp\\ipykernel_34456\\3967408288.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=1000, random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.30%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.54      0.85      0.66      1000\n",
      "     neutral       0.60      0.42      0.50      1000\n",
      "    positive       0.71      0.50      0.59      1000\n",
      "\n",
      "    accuracy                           0.59      3000\n",
      "   macro avg       0.62      0.59      0.58      3000\n",
      "weighted avg       0.62      0.59      0.58      3000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[504 198 298]\n",
      " [137 424 439]\n",
      " [ 70  79 851]]\n"
     ]
    }
   ],
   "source": [
    "# Jika kamu pakai pandas >= 1.3:\n",
    "df1 = pd.read_csv('Dataset Twitter Fix - Indonesian Sentiment Twitter Dataset Labeled (1).csv')\n",
    "df_balanced = (\n",
    "    df1[df1['sentimen'].isin([0, 1, 2])]\n",
    "    .groupby('sentimen', group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=1000, random_state=42))\n",
    ")\n",
    "# Mapping koreksi label salah tulis\n",
    "label_mapping = {\n",
    "    1: 'positive',\n",
    "    2: 'negative',\n",
    "    0: 'neutral'\n",
    "}\n",
    "\n",
    "df_balanced['label'] = df_balanced['sentimen'].replace(label_mapping)\n",
    "\n",
    "sentiments = []\n",
    "\n",
    "for index, row in df_balanced.iterrows():\n",
    "    try:\n",
    "        if pd.isnull(row['Tweet']):\n",
    "            text = str(' ')\n",
    "        else:\n",
    "            text = str(row['Tweet'])\n",
    "        result = hanifnoerr_model(text)[0]['label']\n",
    "    except Exception as e:\n",
    "        result = 'error'\n",
    "    sentiments.append(result)\n",
    "\n",
    "df_balanced['sentiment_result'] = sentiments\n",
    "df_balanced['sentiment_result'] = df_balanced['sentiment_result'].str.lower()\n",
    "\n",
    "# Compare actual and predicted sentiments\n",
    "y_true = df_balanced['label']\n",
    "y_pred = df_balanced['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'neutral', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd2f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (no neutral): 78.64%\n",
      "\n",
      "Classification Report (no neutral):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.88      0.63      0.73       802\n",
      "    negative       0.74      0.92      0.82       921\n",
      "\n",
      "    accuracy                           0.79      1723\n",
      "   macro avg       0.81      0.78      0.78      1723\n",
      "weighted avg       0.80      0.79      0.78      1723\n",
      "\n",
      "\n",
      "Confusion Matrix (no neutral):\n",
      "[[504 298]\n",
      " [ 70 851]]\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where either actual or predicted label is 'neutral'\n",
    "df_balanced = df_balanced[\n",
    "    (df_balanced['label'] != 'neutral') &\n",
    "    (df_balanced['sentiment_result'] != 'neutral')\n",
    "]\n",
    "\n",
    "# Update y_true and y_pred\n",
    "y_true = df_balanced['label']\n",
    "y_pred = df_balanced['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy (no neutral): {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report (no neutral):\")\n",
    "print(classification_report(y_true, y_pred, labels=['positive', 'negative']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (no neutral):\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'negative']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1fa38",
   "metadata": {},
   "source": [
    "crypter70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "018eeaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\POS\\AppData\\Local\\Temp\\ipykernel_34456\\1978757370.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=1000, random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.07%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.79      0.65      1000\n",
      "     neutral       0.60      0.47      0.53      1000\n",
      "    positive       0.66      0.51      0.58      1000\n",
      "\n",
      "    accuracy                           0.59      3000\n",
      "   macro avg       0.60      0.59      0.58      3000\n",
      "weighted avg       0.60      0.59      0.58      3000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[513 207 280]\n",
      " [158 469 373]\n",
      " [101 109 790]]\n"
     ]
    }
   ],
   "source": [
    "# Jika kamu pakai pandas >= 1.3:\n",
    "df1 = pd.read_csv('Dataset Twitter Fix - Indonesian Sentiment Twitter Dataset Labeled (1).csv')\n",
    "df_balanced = (\n",
    "    df1[df1['sentimen'].isin([0, 1, 2])]\n",
    "    .groupby('sentimen', group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=1000, random_state=42))\n",
    ")\n",
    "# Mapping koreksi label salah tulis\n",
    "label_mapping = {\n",
    "    1: 'positive',\n",
    "    2: 'negative',\n",
    "    0: 'neutral'\n",
    "}\n",
    "\n",
    "df_balanced['label'] = df_balanced['sentimen'].replace(label_mapping)\n",
    "\n",
    "sentiments = []\n",
    "\n",
    "for index, row in df_balanced.iterrows():\n",
    "    try:\n",
    "        if pd.isnull(row['Tweet']):\n",
    "            text = str(' ')\n",
    "        else:\n",
    "            text = str(row['Tweet'])\n",
    "        result = crypter70_model(text)[0]['label']\n",
    "    except Exception as e:\n",
    "        result = 'error'\n",
    "    sentiments.append(result)\n",
    "\n",
    "df_balanced['sentiment_result'] = sentiments\n",
    "df_balanced['sentiment_result'] = df_balanced['sentiment_result'].str.lower()\n",
    "\n",
    "# Compare actual and predicted sentiments\n",
    "y_true = df_balanced['label']\n",
    "y_pred = df_balanced['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'neutral', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee032a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (no neutral): 77.38%\n",
      "\n",
      "Classification Report (no neutral):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.84      0.65      0.73       793\n",
      "    negative       0.74      0.89      0.81       891\n",
      "\n",
      "    accuracy                           0.77      1684\n",
      "   macro avg       0.79      0.77      0.77      1684\n",
      "weighted avg       0.78      0.77      0.77      1684\n",
      "\n",
      "\n",
      "Confusion Matrix (no neutral):\n",
      "[[513 280]\n",
      " [101 790]]\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where either actual or predicted label is 'neutral'\n",
    "df_balanced = df_balanced[\n",
    "    (df_balanced['label'] != 'neutral') &\n",
    "    (df_balanced['sentiment_result'] != 'neutral')\n",
    "]\n",
    "\n",
    "# Update y_true and y_pred\n",
    "y_true = df_balanced['label']\n",
    "y_pred = df_balanced['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy (no neutral): {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report (no neutral):\")\n",
    "print(classification_report(y_true, y_pred, labels=['positive', 'negative']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (no neutral):\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'negative']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2460ab",
   "metadata": {},
   "source": [
    "w11wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d5bf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\POS\\AppData\\Local\\Temp\\ipykernel_34456\\1921816966.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=1000, random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.07%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.79      0.63      1000\n",
      "     neutral       0.58      0.41      0.48      1000\n",
      "    positive       0.66      0.51      0.58      1000\n",
      "\n",
      "    accuracy                           0.57      3000\n",
      "   macro avg       0.59      0.57      0.56      3000\n",
      "weighted avg       0.59      0.57      0.56      3000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[511 193 296]\n",
      " [155 409 436]\n",
      " [110  98 792]]\n"
     ]
    }
   ],
   "source": [
    "# Jika kamu pakai pandas >= 1.3:\n",
    "df1 = pd.read_csv('Dataset Twitter Fix - Indonesian Sentiment Twitter Dataset Labeled (1).csv')\n",
    "df_balanced = (\n",
    "    df1[df1['sentimen'].isin([0, 1, 2])]\n",
    "    .groupby('sentimen', group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=1000, random_state=42))\n",
    ")\n",
    "# Mapping koreksi label salah tulis\n",
    "label_mapping = {\n",
    "    1: 'positive',\n",
    "    2: 'negative',\n",
    "    0: 'neutral'\n",
    "}\n",
    "\n",
    "df_balanced['label'] = df_balanced['sentimen'].replace(label_mapping)\n",
    "\n",
    "sentiments = []\n",
    "\n",
    "for index, row in df_balanced.iterrows():\n",
    "    try:\n",
    "        if pd.isnull(row['Tweet']):\n",
    "            text = str(' ')\n",
    "        else:\n",
    "            text = str(row['Tweet'])\n",
    "        result = w11wo_model(text)[0]['label']\n",
    "    except Exception as e:\n",
    "        result = 'error'\n",
    "    sentiments.append(result)\n",
    "\n",
    "df_balanced['sentiment_result'] = sentiments\n",
    "df_balanced['sentiment_result'] = df_balanced['sentiment_result'].str.lower()\n",
    "\n",
    "# Compare actual and predicted sentiments\n",
    "y_true = df_balanced['label']\n",
    "y_pred = df_balanced['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'neutral', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2076c1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (no neutral): 76.24%\n",
      "\n",
      "Classification Report (no neutral):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.82      0.63      0.72       807\n",
      "    negative       0.73      0.88      0.80       902\n",
      "\n",
      "    accuracy                           0.76      1709\n",
      "   macro avg       0.78      0.76      0.76      1709\n",
      "weighted avg       0.77      0.76      0.76      1709\n",
      "\n",
      "\n",
      "Confusion Matrix (no neutral):\n",
      "[[511 296]\n",
      " [110 792]]\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where either actual or predicted label is 'neutral'\n",
    "df_balanced = df_balanced[\n",
    "    (df_balanced['label'] != 'neutral') &\n",
    "    (df_balanced['sentiment_result'] != 'neutral')\n",
    "]\n",
    "\n",
    "# Update y_true and y_pred\n",
    "y_true = df_balanced['label']\n",
    "y_pred = df_balanced['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy (no neutral): {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report (no neutral):\")\n",
    "print(classification_report(y_true, y_pred, labels=['positive', 'negative']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (no neutral):\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'negative']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092bd3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76306f43",
   "metadata": {},
   "source": [
    "HANIFNOERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deba8595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.50%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.82      0.88       200\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       0.97      0.67      0.79       200\n",
      "\n",
      "    accuracy                           0.74       400\n",
      "   macro avg       0.64      0.50      0.56       400\n",
      "weighted avg       0.96      0.74      0.84       400\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[134  58   8]\n",
      " [  0   0   0]\n",
      " [  4  32 164]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\POS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\POS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\POS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Jika kamu pakai pandas >= 1.3:\n",
    "df2 = pd.read_csv('dataset_tweet_sentimen_tayangan_tv.csv')\n",
    "sentiments = []\n",
    "\n",
    "for index, row in df2.iterrows():\n",
    "    try:\n",
    "        if pd.isnull(row['Text Tweet']):\n",
    "            text = str(' ')\n",
    "        else:\n",
    "            text = str(row['Text Tweet'])\n",
    "        result = hanifnoerr_model(text)[0]['label']\n",
    "    except Exception as e:\n",
    "        result = 'error'\n",
    "    sentiments.append(result)\n",
    "\n",
    "df2['sentiment_result'] = sentiments\n",
    "df2['sentiment_result'] = df2['sentiment_result'].str.lower()\n",
    "\n",
    "# Compare actual and predicted sentiments\n",
    "y_true = df2['Sentiment']\n",
    "y_pred = df2['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'neutral', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f3fa031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_result\n",
       "negative    172\n",
       "positive    138\n",
       "neutral      90\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['sentiment_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d77fef08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (no neutral): 96.13%\n",
      "\n",
      "Classification Report (no neutral):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.97      0.94      0.96       142\n",
      "    negative       0.95      0.98      0.96       168\n",
      "\n",
      "    accuracy                           0.96       310\n",
      "   macro avg       0.96      0.96      0.96       310\n",
      "weighted avg       0.96      0.96      0.96       310\n",
      "\n",
      "\n",
      "Confusion Matrix (no neutral):\n",
      "[[134   8]\n",
      " [  4 164]]\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where either actual or predicted label is 'neutral'\n",
    "df2 = df2[\n",
    "    (df2['Sentiment'] != 'neutral') &\n",
    "    (df2['sentiment_result'] != 'neutral')\n",
    "]\n",
    "\n",
    "# Update y_true and y_pred\n",
    "y_true = df2['Sentiment']\n",
    "y_pred = df2['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy (no neutral): {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report (no neutral):\")\n",
    "print(classification_report(y_true, y_pred, labels=['positive', 'negative']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (no neutral):\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'negative']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5796327",
   "metadata": {},
   "source": [
    "crypter70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd090bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.25%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.78      0.85       200\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       0.97      0.65      0.77       200\n",
      "\n",
      "    accuracy                           0.71       400\n",
      "   macro avg       0.64      0.48      0.54       400\n",
      "weighted avg       0.95      0.71      0.81       400\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[129  61  10]\n",
      " [  0   0   0]\n",
      " [  4  40 156]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\POS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\POS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\POS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Jika kamu pakai pandas >= 1.3:\n",
    "df2 = pd.read_csv('dataset_tweet_sentimen_tayangan_tv.csv')\n",
    "sentiments = []\n",
    "\n",
    "for index, row in df2.iterrows():\n",
    "    try:\n",
    "        if pd.isnull(row['Text Tweet']):\n",
    "            text = str(' ')\n",
    "        else:\n",
    "            text = str(row['Text Tweet'])\n",
    "        result = crypter70_model(text)[0]['label']\n",
    "    except Exception as e:\n",
    "        result = 'error'\n",
    "    sentiments.append(result)\n",
    "\n",
    "df2['sentiment_result'] = sentiments\n",
    "df2['sentiment_result'] = df2['sentiment_result'].str.lower()\n",
    "\n",
    "# Compare actual and predicted sentiments\n",
    "y_true = df2['Sentiment']\n",
    "y_pred = df2['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'neutral', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcced6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_result\n",
       "negative    166\n",
       "positive    133\n",
       "neutral     101\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['sentiment_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6927f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (no neutral): 95.32%\n",
      "\n",
      "Classification Report (no neutral):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.97      0.93      0.95       139\n",
      "    negative       0.94      0.97      0.96       160\n",
      "\n",
      "    accuracy                           0.95       299\n",
      "   macro avg       0.95      0.95      0.95       299\n",
      "weighted avg       0.95      0.95      0.95       299\n",
      "\n",
      "\n",
      "Confusion Matrix (no neutral):\n",
      "[[129  10]\n",
      " [  4 156]]\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where either actual or predicted label is 'neutral'\n",
    "df2 = df2[\n",
    "    (df2['Sentiment'] != 'neutral') &\n",
    "    (df2['sentiment_result'] != 'neutral')\n",
    "]\n",
    "\n",
    "# Update y_true and y_pred\n",
    "y_true = df2['Sentiment']\n",
    "y_pred = df2['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy (no neutral): {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report (no neutral):\")\n",
    "print(classification_report(y_true, y_pred, labels=['positive', 'negative']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (no neutral):\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'negative']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18eb1a",
   "metadata": {},
   "source": [
    "w11wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e8b96aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.00%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.80      0.86       200\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       0.92      0.70      0.80       200\n",
      "\n",
      "    accuracy                           0.75       400\n",
      "   macro avg       0.62      0.50      0.55       400\n",
      "weighted avg       0.92      0.75      0.83       400\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[141  47  12]\n",
      " [  0   0   0]\n",
      " [ 13  28 159]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\POS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\POS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\POS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Jika kamu pakai pandas >= 1.3:\n",
    "df2 = pd.read_csv('dataset_tweet_sentimen_tayangan_tv.csv')\n",
    "sentiments = []\n",
    "\n",
    "for index, row in df2.iterrows():\n",
    "    try:\n",
    "        if pd.isnull(row['Text Tweet']):\n",
    "            text = str(' ')\n",
    "        else:\n",
    "            text = str(row['Text Tweet'])\n",
    "        result = w11wo_model(text)[0]['label']\n",
    "    except Exception as e:\n",
    "        result = 'error'\n",
    "    sentiments.append(result)\n",
    "\n",
    "df2['sentiment_result'] = sentiments\n",
    "df2['sentiment_result'] = df2['sentiment_result'].str.lower()\n",
    "\n",
    "# Compare actual and predicted sentiments\n",
    "y_true = df2['Sentiment']\n",
    "y_pred = df2['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'neutral', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e867686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_result\n",
       "negative    171\n",
       "positive    154\n",
       "neutral      75\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['sentiment_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56d08c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (no neutral): 92.31%\n",
      "\n",
      "Classification Report (no neutral):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.92      0.92      0.92       153\n",
      "    negative       0.93      0.92      0.93       172\n",
      "\n",
      "    accuracy                           0.92       325\n",
      "   macro avg       0.92      0.92      0.92       325\n",
      "weighted avg       0.92      0.92      0.92       325\n",
      "\n",
      "\n",
      "Confusion Matrix (no neutral):\n",
      "[[141  12]\n",
      " [ 13 159]]\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where either actual or predicted label is 'neutral'\n",
    "df2 = df2[\n",
    "    (df2['Sentiment'] != 'neutral') &\n",
    "    (df2['sentiment_result'] != 'neutral')\n",
    "]\n",
    "\n",
    "# Update y_true and y_pred\n",
    "y_true = df2['Sentiment']\n",
    "y_pred = df2['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy (no neutral): {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report (no neutral):\")\n",
    "print(classification_report(y_true, y_pred, labels=['positive', 'negative']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (no neutral):\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'negative']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd0906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c35dcb7",
   "metadata": {},
   "source": [
    "HANIFNOERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "244e5b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 27.46%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.09      0.17      3130\n",
      "     neutral       0.22      0.97      0.36      1057\n",
      "    positive       0.77      0.08      0.14       813\n",
      "\n",
      "    accuracy                           0.27      5000\n",
      "   macro avg       0.61      0.38      0.22      5000\n",
      "weighted avg       0.70      0.27      0.20      5000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  63  731   19]\n",
      " [   4 1021   32]\n",
      " [  15 2826  289]]\n"
     ]
    }
   ],
   "source": [
    "df_english = pd.read_csv('Tweets.csv')\n",
    "df_english = df_english.head(5000)\n",
    "sentiments = []\n",
    "\n",
    "for index, row in df_english.iterrows():\n",
    "    try:\n",
    "        if pd.isnull(row['text']):\n",
    "            text = str(' ')\n",
    "        else:\n",
    "            text = str(row['text'])\n",
    "        result = hanifnoerr_model(text)[0]['label']\n",
    "    except Exception as e:\n",
    "        result = 'error'\n",
    "    sentiments.append(result)\n",
    "\n",
    "df_english['sentiment_result'] = sentiments\n",
    "df_english['sentiment_result'] = df_english['sentiment_result'].str.lower()\n",
    "\n",
    "# Compare actual and predicted sentiments\n",
    "y_true = df_english['airline_sentiment']\n",
    "y_pred = df_english['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'neutral', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6306ac73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_result\n",
       "negative    308\n",
       "positive     78\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english['sentiment_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34b2008f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (no neutral): 91.19%\n",
      "\n",
      "Classification Report (no neutral):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.81      0.77      0.79        82\n",
      "    negative       0.94      0.95      0.94       304\n",
      "\n",
      "    accuracy                           0.91       386\n",
      "   macro avg       0.87      0.86      0.87       386\n",
      "weighted avg       0.91      0.91      0.91       386\n",
      "\n",
      "\n",
      "Confusion Matrix (no neutral):\n",
      "[[ 63  19]\n",
      " [ 15 289]]\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where either actual or predicted label is 'neutral'\n",
    "df_english = df_english[\n",
    "    (df_english['airline_sentiment'] != 'neutral') &\n",
    "    (df_english['sentiment_result'] != 'neutral')\n",
    "]\n",
    "\n",
    "# Update y_true and y_pred\n",
    "y_true = df_english['airline_sentiment']\n",
    "y_pred = df_english['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy (no neutral): {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report (no neutral):\")\n",
    "print(classification_report(y_true, y_pred, labels=['positive', 'negative']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (no neutral):\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'negative']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1039793",
   "metadata": {},
   "source": [
    "crypter70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4940120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 25.50%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.05      0.09      3130\n",
      "     neutral       0.22      0.96      0.36      1057\n",
      "    positive       0.78      0.14      0.24       813\n",
      "\n",
      "    accuracy                           0.26      5000\n",
      "   macro avg       0.58      0.38      0.23      5000\n",
      "weighted avg       0.65      0.26      0.17      5000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 116  679   18]\n",
      " [  12 1017   28]\n",
      " [  21 2967  142]]\n"
     ]
    }
   ],
   "source": [
    "df_english = pd.read_csv('Tweets.csv')\n",
    "df_english = df_english.head(5000)\n",
    "sentiments = []\n",
    "\n",
    "for index, row in df_english.iterrows():\n",
    "    try:\n",
    "        if pd.isnull(row['text']):\n",
    "            text = str(' ')\n",
    "        else:\n",
    "            text = str(row['text'])\n",
    "        result = crypter70_model(text)[0]['label']\n",
    "    except Exception as e:\n",
    "        result = 'error'\n",
    "    sentiments.append(result)\n",
    "\n",
    "df_english['sentiment_result'] = sentiments\n",
    "df_english['sentiment_result'] = df_english['sentiment_result'].str.lower()\n",
    "\n",
    "# Compare actual and predicted sentiments\n",
    "y_true = df_english['airline_sentiment']\n",
    "y_pred = df_english['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'neutral', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92c4688e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_result\n",
       "neutral     4663\n",
       "negative     188\n",
       "positive     149\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english['sentiment_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f872234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (no neutral): 86.87%\n",
      "\n",
      "Classification Report (no neutral):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.85      0.87      0.86       134\n",
      "    negative       0.89      0.87      0.88       163\n",
      "\n",
      "    accuracy                           0.87       297\n",
      "   macro avg       0.87      0.87      0.87       297\n",
      "weighted avg       0.87      0.87      0.87       297\n",
      "\n",
      "\n",
      "Confusion Matrix (no neutral):\n",
      "[[116  18]\n",
      " [ 21 142]]\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where either actual or predicted label is 'neutral'\n",
    "df_english = df_english[\n",
    "    (df_english['airline_sentiment'] != 'neutral') &\n",
    "    (df_english['sentiment_result'] != 'neutral')\n",
    "]\n",
    "\n",
    "# Update y_true and y_pred\n",
    "y_true = df_english['airline_sentiment']\n",
    "y_pred = df_english['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy (no neutral): {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report (no neutral):\")\n",
    "print(classification_report(y_true, y_pred, labels=['positive', 'negative']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (no neutral):\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'negative']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33934d19",
   "metadata": {},
   "source": [
    "w11wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "945f177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.80%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.76      0.76      3130\n",
      "     neutral       0.44      0.35      0.39      1057\n",
      "    positive       0.47      0.61      0.53       813\n",
      "\n",
      "    accuracy                           0.65      5000\n",
      "   macro avg       0.56      0.57      0.56      5000\n",
      "weighted avg       0.65      0.65      0.64      5000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 494  106  213]\n",
      " [ 157  365  535]\n",
      " [ 399  350 2381]]\n"
     ]
    }
   ],
   "source": [
    "df_english = pd.read_csv('Tweets.csv')\n",
    "df_english = df_english.head(5000)\n",
    "sentiments = []\n",
    "\n",
    "for index, row in df_english.iterrows():\n",
    "    try:\n",
    "        if pd.isnull(row['text']):\n",
    "            text = str(' ')\n",
    "        else:\n",
    "            text = str(row['text'])\n",
    "        result = w11wo_model(text)[0]['label']\n",
    "    except Exception as e:\n",
    "        result = 'error'\n",
    "    sentiments.append(result)\n",
    "\n",
    "df_english['sentiment_result'] = sentiments\n",
    "df_english['sentiment_result'] = df_english['sentiment_result'].str.lower()\n",
    "\n",
    "# Compare actual and predicted sentiments\n",
    "y_true = df_english['airline_sentiment']\n",
    "y_pred = df_english['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'neutral', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53a474f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_result\n",
       "negative    3129\n",
       "positive    1050\n",
       "neutral      821\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english['sentiment_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "686fa6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (no neutral): 82.45%\n",
      "\n",
      "Classification Report (no neutral):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.55      0.70      0.62       707\n",
      "    negative       0.92      0.86      0.89      2780\n",
      "\n",
      "    accuracy                           0.82      3487\n",
      "   macro avg       0.74      0.78      0.75      3487\n",
      "weighted avg       0.84      0.82      0.83      3487\n",
      "\n",
      "\n",
      "Confusion Matrix (no neutral):\n",
      "[[ 494  213]\n",
      " [ 399 2381]]\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where either actual or predicted label is 'neutral'\n",
    "df_english = df_english[\n",
    "    (df_english['airline_sentiment'] != 'neutral') &\n",
    "    (df_english['sentiment_result'] != 'neutral')\n",
    "]\n",
    "\n",
    "# Update y_true and y_pred\n",
    "y_true = df_english['airline_sentiment']\n",
    "y_pred = df_english['sentiment_result']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy (no neutral): {accuracy:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report (no neutral):\")\n",
    "print(classification_report(y_true, y_pred, labels=['positive', 'negative']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (no neutral):\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['positive', 'negative']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d09c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
